{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rwa_model import RWA\n",
    "from utils import AddTask\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard_logger import configure, log_value\n",
    "\n",
    "configure(\"training/run1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "batch = 4\n",
    "rwa = RWA(num_inputs, 250, 1, fwd_type=\"cumulative\")\n",
    "\n",
    "rwa.train()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print_steps = 10\n",
    "\n",
    "current_lr = 1e-3\n",
    "optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "\n",
    "running_loss = 0.0\n",
    "accumulated_loss = []\n",
    "\n",
    "test = AddTask(100000, 10000, 100)\n",
    "\n",
    "data_loader = DataLoader(test, batch_size=batch, shuffle=True, num_workers=4)\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        n, d, h, a_max = rwa.init_internal(batch)  # does this go inside this loop? I think so.\n",
    "\n",
    "        rwa.zero_grad()\n",
    "        outputs, n, d, h, a_max = rwa(inputs, n, d, h, a_max)\n",
    "\n",
    "        n = Variable(n.data)\n",
    "        d = Variable(d.data)\n",
    "        h = Variable(h.data)\n",
    "        a_max = Variable(a_max.data)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        accumulated_loss.append(loss.data[0])\n",
    "\n",
    "        if i % print_steps == print_steps-1:\n",
    "            current_step = i + 1 + len(data_loader) * epoch\n",
    "            log_value(\"Loss\", running_loss/print_steps, step=current_step)\n",
    "            log_value(\"LR\", current_lr, step=current_step)\n",
    "            log_value(\"Error\", np.abs(outputs.data[0, 0] - labels.data[0, 0]), step=current_step)\n",
    "\n",
    "            if np.mean(np.abs(np.diff(accumulated_loss))) <= current_lr:  # this isn't working?\n",
    "                torch.save(rwa.state_dict(), \"models/add.dat\")\n",
    "                current_lr = max([current_lr * 1e-1, 1e-8])\n",
    "                print(\"lr decayed to: \", current_lr)\n",
    "                optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "                accumulated_loss.clear()\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "torch.save(rwa.state_dict(), \"models/rwa_add.dat\")\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}