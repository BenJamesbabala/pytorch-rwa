{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rwa_model import RWA\n",
    "from utils import AddTask\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard_logger import configure, log_value\n",
    "\n",
    "configure(\"training/fix_init_factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step:  5 Loss:  1.1824751615524292\n",
      "Current step:  10 Loss:  1.1605567693710328\n",
      "Current step:  15 Loss:  1.090411901473999\n",
      "Current step:  20 Loss:  1.0715124011039734\n",
      "Current step:  25 Loss:  1.0391909599304199\n",
      "Current step:  30 Loss:  0.869539451599121\n",
      "Current step:  35 Loss:  0.8671739339828491\n",
      "Current step:  40 Loss:  0.5855353832244873\n",
      "Current step:  45 Loss:  0.33125067949295045\n",
      "Current step:  50 Loss:  0.20864032506942748\n",
      "Current step:  55 Loss:  0.26924313306808473\n",
      "Current step:  60 Loss:  0.31446107625961306\n",
      "Current step:  65 Loss:  0.3031612396240234\n",
      "Current step:  70 Loss:  0.2564889550209045\n",
      "Current step:  75 Loss:  0.21177843809127808\n",
      "Current step:  80 Loss:  0.18446245789527893\n",
      "Current step:  85 Loss:  0.18372851610183716\n",
      "Current step:  90 Loss:  0.1630633920431137\n",
      "Current step:  95 Loss:  0.18584435284137726\n",
      "Current step:  100 Loss:  0.1608738750219345\n",
      "Current step:  105 Loss:  0.18243353068828583\n",
      "Current step:  110 Loss:  0.2078883409500122\n",
      "Current step:  115 Loss:  0.17025100886821748\n",
      "Current step:  120 Loss:  0.17496936917304992\n",
      "Current step:  125 Loss:  0.16524848341941833\n",
      "Current step:  130 Loss:  0.17437783181667327\n",
      "Current step:  135 Loss:  0.15962683856487275\n",
      "Current step:  140 Loss:  0.16699663996696473\n",
      "Current step:  145 Loss:  0.1751103311777115\n",
      "Current step:  150 Loss:  0.15519801080226897\n",
      "Current step:  155 Loss:  0.17465138137340547\n",
      "Current step:  160 Loss:  0.17041796445846558\n",
      "Current step:  165 Loss:  0.1689620018005371\n",
      "Current step:  170 Loss:  0.15297751724720002\n",
      "Current step:  175 Loss:  0.1704903721809387\n",
      "Current step:  180 Loss:  0.16959028244018554\n",
      "Current step:  185 Loss:  0.15757279992103576\n",
      "Current step:  190 Loss:  0.15799586325883866\n",
      "Current step:  195 Loss:  0.15195268988609315\n",
      "Current step:  200 Loss:  0.16461800336837767\n",
      "Current step:  205 Loss:  0.18101072907447815\n",
      "Current step:  210 Loss:  0.17434561550617217\n",
      "Current step:  215 Loss:  0.1636625796556473\n",
      "Current step:  220 Loss:  0.16206970512866975\n",
      "Current step:  225 Loss:  0.17100065648555757\n",
      "Current step:  230 Loss:  0.17768982648849488\n",
      "Current step:  235 Loss:  0.1612385630607605\n",
      "Current step:  240 Loss:  0.1543131113052368\n",
      "Current step:  245 Loss:  0.16923797726631165\n",
      "Current step:  250 Loss:  0.15795404613018035\n",
      "Current step:  255 Loss:  0.16973269283771514\n",
      "Current step:  260 Loss:  0.1660842627286911\n",
      "Current step:  265 Loss:  0.17887671291828156\n",
      "Current step:  270 Loss:  0.16148800849914552\n",
      "Current step:  275 Loss:  0.1806923121213913\n",
      "Current step:  280 Loss:  0.15411269962787627\n",
      "Current step:  285 Loss:  0.17588338851928711\n",
      "Current step:  290 Loss:  0.18234786689281463\n",
      "Current step:  295 Loss:  0.1748759001493454\n",
      "Current step:  300 Loss:  0.16505142152309418\n",
      "Current step:  305 Loss:  0.1803881198167801\n",
      "Current step:  310 Loss:  0.1806030124425888\n",
      "Current step:  315 Loss:  0.17037911713123322\n",
      "Current step:  320 Loss:  0.18147790431976318\n",
      "Current step:  325 Loss:  0.1756644457578659\n",
      "Current step:  330 Loss:  0.16288071870803833\n",
      "Current step:  335 Loss:  0.17199270725250243\n",
      "Current step:  340 Loss:  0.15896569490432738\n",
      "Current step:  345 Loss:  0.1600703090429306\n",
      "Current step:  350 Loss:  0.15335722267627716\n",
      "Current step:  355 Loss:  0.15234950184822083\n",
      "Current step:  360 Loss:  0.17761418223381042\n",
      "Current step:  365 Loss:  0.16348021030426024\n",
      "Current step:  370 Loss:  0.18061579763889313\n",
      "Current step:  375 Loss:  0.16072366535663604\n",
      "Current step:  380 Loss:  0.1647024780511856\n",
      "Current step:  385 Loss:  0.15962982326745986\n",
      "Current step:  390 Loss:  0.16316335499286652\n",
      "Current step:  395 Loss:  0.1584872543811798\n",
      "Current step:  400 Loss:  0.15412189662456513\n",
      "Current step:  405 Loss:  0.17001571357250214\n",
      "Current step:  410 Loss:  0.15954067707061767\n",
      "Current step:  415 Loss:  0.15776985883712769\n",
      "Current step:  420 Loss:  0.17073064744472505\n",
      "Current step:  425 Loss:  0.1552150219678879\n",
      "Current step:  430 Loss:  0.16373418569564818\n",
      "Current step:  435 Loss:  0.16344930827617646\n",
      "Current step:  440 Loss:  0.17379975616931914\n",
      "Current step:  445 Loss:  0.16714785397052764\n",
      "Current step:  450 Loss:  0.15856240391731263\n",
      "Current step:  455 Loss:  0.16857390105724335\n",
      "Current step:  460 Loss:  0.16661929488182067\n",
      "Current step:  465 Loss:  0.16928892731666564\n",
      "Current step:  470 Loss:  0.16900743842124938\n",
      "Current step:  475 Loss:  0.1843113511800766\n",
      "Current step:  480 Loss:  0.17292118370532988\n",
      "Current step:  485 Loss:  0.18119603991508484\n",
      "Current step:  490 Loss:  0.16938067674636842\n",
      "Current step:  495 Loss:  0.15565039217472076\n",
      "Current step:  500 Loss:  0.17225025296211244\n",
      "Current step:  505 Loss:  0.17875038385391234\n",
      "Current step:  510 Loss:  0.1610901892185211\n",
      "Current step:  515 Loss:  0.16789659559726716\n",
      "Current step:  520 Loss:  0.165766841173172\n",
      "Current step:  525 Loss:  0.16193131506443023\n",
      "Current step:  530 Loss:  0.16215124130249023\n",
      "Current step:  535 Loss:  0.14668097645044326\n",
      "Current step:  540 Loss:  0.16310043036937713\n",
      "Current step:  545 Loss:  0.16592957377433776\n",
      "Current step:  550 Loss:  0.1655232310295105\n"
     ]
    }
   ],
   "source": [
    "num_features = 2\n",
    "num_classes = 1\n",
    "num_cells = 250\n",
    "batch = 100\n",
    "rwa = RWA(num_features, num_cells, num_classes, fwd_type=\"cumulative\")  # something is bad\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print_steps = 5\n",
    "\n",
    "current_lr = 0.001\n",
    "\n",
    "running_loss = 0.0\n",
    "time_since_decay = 0\n",
    "accumulated_loss = []\n",
    "\n",
    "test = AddTask(100000, 10000, 100)\n",
    "\n",
    "data_loader = DataLoader(test, batch_size=batch, shuffle=True, num_workers=4)\n",
    "\n",
    "s, n, d, h, a_max = rwa.init_sndha(batch)\n",
    "rwa.register_parameter('s', s)  # make sure s changes after each optimizer step\n",
    "\n",
    "optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "\n",
    "rwa.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        rwa.zero_grad()\n",
    "        outputs, rwa.s, n_new, d_new, h_new, a_newmax = rwa(inputs, rwa.s, n, d, h, a_max)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n = Variable(n_new.data)\n",
    "        d = Variable(d_new.data)\n",
    "        h = Variable(h_new.data)\n",
    "        a_max = Variable(a_newmax.data)\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        accumulated_loss.append(loss.data[0])\n",
    "        time_since_decay += 1\n",
    "\n",
    "        if i % print_steps == print_steps-1:\n",
    "            current_step = i + 1 + len(data_loader) * epoch\n",
    "            print(\"Current step: \", current_step, \"Loss: \", running_loss / print_steps)\n",
    "            log_value(\"Loss\", running_loss / print_steps, step=current_step)\n",
    "            log_value(\"LR\", current_lr, step=current_step)\n",
    "            log_value(\"Error\", np.abs(outputs.data[0, 0] - labels.data[0, 0]), step=current_step)\n",
    "            log_value(\"Output\", outputs.data[0, 0], step=current_step)\n",
    "            \n",
    "#             if np.abs(np.mean(np.diff(accumulated_loss))) <= current_lr:\n",
    "#                 torch.save(rwa.state_dict(), \"models/add.dat\")\n",
    "#                 current_lr = max([current_lr * 1e-1, 1e-8])\n",
    "#                 print(\"lr decayed to: \", current_lr)\n",
    "#                 optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "#                 accumulated_loss.clear()\n",
    "#                 time_since_decay = 0\n",
    "                \n",
    "#                 if np.mean(accumulated_loss) >= 0.165:\n",
    "#                     torch.save(rwa.state_dict(), \"models/add.dat\")\n",
    "#                     current_lr = min([current_lr * 2, 1e-2])\n",
    "#                     print(\"lr decayed to: \", current_lr)\n",
    "#                     optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "#                     accumulated_loss.clear()\n",
    "#                     time_since_decay = 0\n",
    "                    \n",
    "#                 else:\n",
    "#                     torch.save(rwa.state_dict(), \"models/add.dat\")\n",
    "#                     current_lr = max([current_lr * 1e-1, 1e-8])\n",
    "#                     print(\"lr decayed to: \", current_lr)\n",
    "#                     optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "#                     accumulated_loss.clear()\n",
    "#                     time_since_decay = 0\n",
    "                \n",
    "                \n",
    "            running_loss = 0.0\n",
    "\n",
    "torch.save(rwa.state_dict(), \"models/rwa_add.dat\")\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
