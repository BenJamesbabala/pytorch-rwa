{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rwa_model import RWA\n",
    "from utils import AddTask\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard_logger import configure, log_value\n",
    "\n",
    "configure(\"training/seq_fix_nda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step:  5 Loss:  1.1222824811935426\n",
      "Current step:  10 Loss:  1.1456548690795898\n",
      "Current step:  15 Loss:  1.1367244720458984\n",
      "Current step:  20 Loss:  1.0733978033065796\n",
      "Current step:  25 Loss:  1.0184694528579712\n",
      "Current step:  30 Loss:  0.9759223341941834\n",
      "Current step:  35 Loss:  0.7731917142868042\n",
      "Current step:  40 Loss:  0.6118639290332795\n",
      "Current step:  45 Loss:  0.3399844765663147\n",
      "Current step:  50 Loss:  0.1889703929424286\n",
      "Current step:  55 Loss:  0.22958042621612548\n",
      "Current step:  60 Loss:  0.3507343053817749\n",
      "Current step:  65 Loss:  0.3031211316585541\n",
      "Current step:  70 Loss:  0.23396671712398528\n",
      "Current step:  75 Loss:  0.21118595600128173\n",
      "Current step:  80 Loss:  0.18122841119766236\n",
      "Current step:  85 Loss:  0.16833272874355315\n"
     ]
    }
   ],
   "source": [
    "num_features = 2\n",
    "num_classes = 1\n",
    "num_cells = 250\n",
    "batch = 100\n",
    "rwa = RWA(num_features, num_cells, num_classes, fwd_type=\"cumulative\")  # something is bad\n",
    "\n",
    "rwa.train()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print_steps = 5\n",
    "\n",
    "current_lr = 0.001\n",
    "\n",
    "running_loss = 0.0\n",
    "time_since_decay = 0\n",
    "accumulated_loss = []\n",
    "\n",
    "test = AddTask(100000, 10000, 100)\n",
    "\n",
    "data_loader = DataLoader(test, batch_size=batch, shuffle=True, num_workers=4)\n",
    "\n",
    "s, n, d, h, a_max = rwa.init_sndha(batch)\n",
    "# rwa.register_parameter('s', s)  # make sure s changes after each optimizer step\n",
    "\n",
    "optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        rwa.zero_grad()\n",
    "        outputs, s_new, n_new, d_new, h_new, a_newmax = rwa(inputs, s, n, d, h, a_max)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        s = Variable(s_new.data)\n",
    "        n = Variable(n_new.data)\n",
    "        d = Variable(d_new.data)\n",
    "        h = Variable(h_new.data)\n",
    "        a_max = Variable(a_newmax.data)\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        accumulated_loss.append(loss.data[0])\n",
    "        time_since_decay += 1\n",
    "\n",
    "        if i % print_steps == print_steps-1:\n",
    "            current_step = i + 1 + len(data_loader) * epoch\n",
    "            print(\"Current step: \", current_step, \"Loss: \", running_loss / print_steps)\n",
    "            log_value(\"Loss\", running_loss / print_steps, step=current_step)\n",
    "            log_value(\"LR\", current_lr, step=current_step)\n",
    "            log_value(\"Error\", np.abs(outputs.data[0, 0] - labels.data[0, 0]), step=current_step)\n",
    "            log_value(\"Output\", outputs.data[0, 0], step=current_step)\n",
    "\n",
    "#             if time_since_decay >= 0.3 * len(data_loader):\n",
    "#                 if np.abs(np.mean(np.diff(accumulated_loss))) <= current_lr:\n",
    "#                     torch.save(rwa.state_dict(), \"models/add.dat\")\n",
    "#                     current_lr = max([current_lr * 2e-1, 1e-8])\n",
    "#                     print(\"lr decayed to: \", current_lr)\n",
    "#                     optimizer = optim.Adam(rwa.parameters(), lr=current_lr, weight_decay=5e-4)\n",
    "#                     accumulated_loss.clear()\n",
    "#                     time_since_decay = 0\n",
    "                \n",
    "            running_loss = 0.0\n",
    "\n",
    "torch.save(rwa.state_dict(), \"models/rwa_add.dat\")\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
