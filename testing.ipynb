{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rwa_model import RWA\n",
    "from utils import AddTask\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard_logger import configure, log_value\n",
    "\n",
    "configure(\"training/lr_decay_low_diff_dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step:  5 Loss:  0.6890386506915093\n",
      "Current step:  10 Loss:  0.6728095769882202\n",
      "Current step:  15 Loss:  0.5481717526912689\n",
      "Current step:  20 Loss:  0.40343273878097535\n",
      "Current step:  25 Loss:  0.2074165940284729\n",
      "Current step:  30 Loss:  0.2280484437942505\n",
      "Current step:  35 Loss:  0.17410408854484558\n",
      "Current step:  40 Loss:  0.23224450647830963\n",
      "Current step:  45 Loss:  0.1907748430967331\n",
      "Current step:  50 Loss:  0.17685036063194276\n",
      "Current step:  55 Loss:  0.17783498466014863\n",
      "Current step:  60 Loss:  0.16261211037635803\n",
      "Current step:  65 Loss:  0.17822770178318023\n",
      "Current step:  70 Loss:  0.1659500002861023\n",
      "Current step:  75 Loss:  0.18270912766456604\n",
      "Current step:  80 Loss:  0.16471443176269532\n",
      "Current step:  85 Loss:  0.16926828622817994\n",
      "Current step:  90 Loss:  0.15116122215986252\n",
      "Current step:  95 Loss:  0.18050384819507598\n",
      "Current step:  100 Loss:  0.16543774902820588\n",
      "Current step:  105 Loss:  0.17910849153995514\n",
      "lr decayed to:  0.001\n",
      "Current step:  110 Loss:  0.16986500322818757\n",
      "Current step:  115 Loss:  0.15893442630767823\n",
      "Current step:  120 Loss:  0.173397034406662\n",
      "lr decayed to:  0.0001\n",
      "Current step:  125 Loss:  0.17464370727539064\n",
      "Current step:  130 Loss:  0.17287854850292206\n",
      "Current step:  135 Loss:  0.17334335148334504\n",
      "Current step:  140 Loss:  0.1671684592962265\n",
      "Current step:  145 Loss:  0.18214454650878906\n",
      "Current step:  150 Loss:  0.16478666067123413\n",
      "Current step:  155 Loss:  0.16160673201084136\n",
      "Current step:  160 Loss:  0.1723380595445633\n",
      "Current step:  165 Loss:  0.17538111209869384\n",
      "Current step:  170 Loss:  0.15940743386745454\n",
      "Current step:  175 Loss:  0.1741204410791397\n",
      "Current step:  180 Loss:  0.16511360704898834\n",
      "Current step:  185 Loss:  0.16952674984931945\n",
      "Current step:  190 Loss:  0.1736518532037735\n",
      "Current step:  195 Loss:  0.16802825182676315\n",
      "Current step:  200 Loss:  0.16816416382789612\n",
      "Current step:  205 Loss:  0.16540498435497283\n",
      "lr decayed to:  1e-05\n",
      "Current step:  210 Loss:  0.17238558530807496\n",
      "Current step:  215 Loss:  0.16990532279014586\n",
      "Current step:  220 Loss:  0.17830943465232849\n",
      "Current step:  225 Loss:  0.17131564021110535\n",
      "Current step:  230 Loss:  0.1632276028394699\n",
      "Current step:  235 Loss:  0.16500167846679686\n",
      "Current step:  240 Loss:  0.17881894409656524\n",
      "Current step:  245 Loss:  0.17503781914710997\n",
      "Current step:  250 Loss:  0.16256545186042787\n",
      "Current step:  255 Loss:  0.174006786942482\n",
      "Current step:  260 Loss:  0.15803361535072327\n",
      "Current step:  265 Loss:  0.17977263331413268\n",
      "Current step:  270 Loss:  0.1666686415672302\n",
      "Current step:  275 Loss:  0.16500794887542725\n",
      "Current step:  280 Loss:  0.1729642629623413\n",
      "Current step:  285 Loss:  0.16599973142147065\n",
      "Current step:  290 Loss:  0.15391804575920104\n",
      "Current step:  295 Loss:  0.1639006495475769\n",
      "Current step:  300 Loss:  0.1620533883571625\n",
      "Current step:  305 Loss:  0.174136021733284\n",
      "Current step:  310 Loss:  0.16332848668098449\n",
      "Current step:  315 Loss:  0.15348202884197235\n",
      "Current step:  320 Loss:  0.17043737173080445\n",
      "Current step:  325 Loss:  0.16208386719226836\n",
      "Current step:  330 Loss:  0.16841755509376527\n",
      "Current step:  335 Loss:  0.1645307719707489\n",
      "Current step:  340 Loss:  0.15883568674325943\n",
      "Current step:  345 Loss:  0.1586891233921051\n",
      "Current step:  350 Loss:  0.15177028775215148\n",
      "Current step:  355 Loss:  0.16089855432510375\n",
      "Current step:  360 Loss:  0.1807079792022705\n",
      "Current step:  365 Loss:  0.17007807195186614\n",
      "Current step:  370 Loss:  0.16758484542369842\n",
      "Current step:  375 Loss:  0.18109738528728486\n",
      "Current step:  380 Loss:  0.1754319041967392\n",
      "Current step:  385 Loss:  0.16381855309009552\n",
      "lr decayed to:  1.0000000000000002e-06\n",
      "Current step:  390 Loss:  0.15734438896179198\n",
      "Current step:  395 Loss:  0.16418760418891906\n",
      "Current step:  400 Loss:  0.15628854930400848\n",
      "Current step:  405 Loss:  0.1711473733186722\n",
      "Current step:  410 Loss:  0.15520356595516205\n",
      "Current step:  415 Loss:  0.16750505268573762\n",
      "Current step:  420 Loss:  0.16662953197956085\n",
      "Current step:  425 Loss:  0.17662131488323213\n",
      "Current step:  430 Loss:  0.17532912790775299\n",
      "Current step:  435 Loss:  0.1713074892759323\n",
      "Current step:  440 Loss:  0.17299134135246277\n",
      "Current step:  445 Loss:  0.15942954123020173\n",
      "Current step:  450 Loss:  0.1920780211687088\n",
      "Current step:  455 Loss:  0.1875285267829895\n",
      "Current step:  460 Loss:  0.1615769535303116\n",
      "Current step:  465 Loss:  0.16719282269477845\n",
      "Current step:  470 Loss:  0.17965567111968994\n",
      "Current step:  475 Loss:  0.1576585978269577\n",
      "Current step:  480 Loss:  0.16854199171066284\n",
      "Current step:  485 Loss:  0.16072956025600432\n",
      "Current step:  490 Loss:  0.1729628175497055\n",
      "Current step:  495 Loss:  0.1679361045360565\n",
      "Current step:  500 Loss:  0.16332921385765076\n",
      "Current step:  505 Loss:  0.17586531341075898\n",
      "Current step:  510 Loss:  0.15344681441783906\n",
      "Current step:  515 Loss:  0.16577186584472656\n",
      "Current step:  520 Loss:  0.18032206594944\n",
      "Current step:  525 Loss:  0.15598415732383727\n",
      "Current step:  530 Loss:  0.16284044086933136\n",
      "Current step:  535 Loss:  0.1614741712808609\n",
      "Current step:  540 Loss:  0.15961819291114807\n",
      "Current step:  545 Loss:  0.17342734038829805\n",
      "Current step:  550 Loss:  0.17118701338768005\n",
      "Current step:  555 Loss:  0.17283745408058165\n",
      "Current step:  560 Loss:  0.1634799689054489\n",
      "Current step:  565 Loss:  0.16579068303108216\n",
      "Current step:  570 Loss:  0.15959229767322541\n",
      "Current step:  575 Loss:  0.15303225219249725\n",
      "Current step:  580 Loss:  0.16033503413200378\n",
      "Current step:  585 Loss:  0.17013715505599974\n",
      "Current step:  590 Loss:  0.18547880947589873\n",
      "Current step:  595 Loss:  0.15241285264492035\n",
      "Current step:  600 Loss:  0.16569769084453584\n",
      "Current step:  605 Loss:  0.17878920137882232\n",
      "Current step:  610 Loss:  0.16409577131271363\n",
      "Current step:  615 Loss:  0.17946158349514008\n",
      "Current step:  620 Loss:  0.17202032804489137\n",
      "Current step:  625 Loss:  0.17624495923519135\n",
      "Current step:  630 Loss:  0.16776128113269806\n",
      "Current step:  635 Loss:  0.1670730620622635\n",
      "Current step:  640 Loss:  0.17511391043663024\n",
      "Current step:  645 Loss:  0.1712609738111496\n",
      "Current step:  650 Loss:  0.18112516403198242\n",
      "Current step:  655 Loss:  0.1789950430393219\n",
      "Current step:  660 Loss:  0.17228586971759796\n",
      "Current step:  665 Loss:  0.1691569060087204\n",
      "Current step:  670 Loss:  0.1773831605911255\n",
      "Current step:  675 Loss:  0.16480663120746614\n",
      "Current step:  680 Loss:  0.15966916382312774\n",
      "Current step:  685 Loss:  0.17217641174793244\n",
      "Current step:  690 Loss:  0.16584373116493226\n",
      "Current step:  695 Loss:  0.1526605635881424\n",
      "Current step:  700 Loss:  0.17670985758304597\n",
      "Current step:  705 Loss:  0.16811131238937377\n",
      "Current step:  710 Loss:  0.16167854964733125\n",
      "Current step:  715 Loss:  0.169578018784523\n",
      "Current step:  720 Loss:  0.16304131150245665\n",
      "Current step:  725 Loss:  0.18387194871902465\n",
      "Current step:  730 Loss:  0.17353737056255342\n",
      "Current step:  735 Loss:  0.1730618268251419\n",
      "Current step:  740 Loss:  0.1775122970342636\n",
      "Current step:  745 Loss:  0.15832963287830354\n",
      "Current step:  750 Loss:  0.1719851851463318\n",
      "Current step:  755 Loss:  0.15658003389835357\n",
      "Current step:  760 Loss:  0.16216438114643097\n",
      "Current step:  765 Loss:  0.15655426383018495\n",
      "Current step:  770 Loss:  0.1707915633916855\n",
      "Current step:  775 Loss:  0.18267227113246917\n",
      "Current step:  780 Loss:  0.1603393852710724\n",
      "Current step:  785 Loss:  0.16437385380268096\n",
      "Current step:  790 Loss:  0.15449124872684478\n",
      "Current step:  795 Loss:  0.16211015582084656\n"
     ]
    }
   ],
   "source": [
    "num_features = 2\n",
    "num_classes = 1\n",
    "num_cells = 250\n",
    "batch = 100\n",
    "rwa = RWA(num_features, num_cells, num_classes, fwd_type=\"cumulative\")  # something is bad\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print_steps = 5\n",
    "\n",
    "current_lr = 0.01\n",
    "\n",
    "running_loss = 0.0\n",
    "time_since_decay = 0\n",
    "accumulated_loss = []\n",
    "\n",
    "test = AddTask(100000, 10000, 100)\n",
    "\n",
    "data_loader = DataLoader(test, batch_size=batch, shuffle=True, num_workers=4)\n",
    "\n",
    "s, n, d, h, a_max = rwa.init_sndha(batch)\n",
    "rwa.register_parameter('s', s)  # make sure s changes after each optimizer step\n",
    "\n",
    "optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "\n",
    "rwa.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        rwa.zero_grad()\n",
    "        outputs, rwa.s, n_new, d_new, h_new, a_newmax = rwa(inputs, rwa.s, n, d, h, a_max)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n = Variable(n_new.data)\n",
    "        d = Variable(d_new.data)\n",
    "        h = Variable(h_new.data)\n",
    "        a_max = Variable(a_newmax.data)\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        accumulated_loss.append(loss.data[0])\n",
    "        time_since_decay += 1\n",
    "\n",
    "        if i % print_steps == print_steps-1:\n",
    "            current_step = i + 1 + len(data_loader) * epoch\n",
    "            print(\"Current step: \", current_step, \"Loss: \", running_loss / print_steps)\n",
    "            log_value(\"Loss\", running_loss / print_steps, step=current_step)\n",
    "            log_value(\"LR\", current_lr, step=current_step)\n",
    "            log_value(\"Error\", np.abs(outputs.data[0, 0] - labels.data[0, 0]), step=current_step)\n",
    "            log_value(\"Output\", outputs.data[0, 0], step=current_step)\n",
    "            \n",
    "            if np.abs(np.mean(np.diff(accumulated_loss))) <= current_lr:\n",
    "                torch.save(rwa.state_dict(), \"models/add.dat\")\n",
    "                current_lr = max([current_lr * 1e-1, 1e-8])\n",
    "                print(\"lr decayed to: \", current_lr)\n",
    "                optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "                accumulated_loss.clear()\n",
    "                time_since_decay = 0\n",
    "                \n",
    "#                 if np.mean(accumulated_loss) >= 0.165:\n",
    "#                     torch.save(rwa.state_dict(), \"models/add.dat\")\n",
    "#                     current_lr = min([current_lr * 2, 1e-2])\n",
    "#                     print(\"lr decayed to: \", current_lr)\n",
    "#                     optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "#                     accumulated_loss.clear()\n",
    "#                     time_since_decay = 0\n",
    "                    \n",
    "#                 else:\n",
    "#                     torch.save(rwa.state_dict(), \"models/add.dat\")\n",
    "#                     current_lr = max([current_lr * 1e-1, 1e-8])\n",
    "#                     print(\"lr decayed to: \", current_lr)\n",
    "#                     optimizer = optim.Adam(rwa.parameters(), lr=current_lr)\n",
    "#                     accumulated_loss.clear()\n",
    "#                     time_since_decay = 0\n",
    "                \n",
    "                \n",
    "            running_loss = 0.0\n",
    "\n",
    "torch.save(rwa.state_dict(), \"models/rwa_add.dat\")\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
